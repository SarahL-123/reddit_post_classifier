{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "Using .json API as well as PRAW, scrape some data from reddit.\n",
    "\n",
    "## Using json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally I would want to use PRAW or something more user friendly but for the purposes of this assignment I just left it as the .json (although it's not very hard to parse it anyway).\n",
    "\n",
    "In any case the scraper will record 3 things:\n",
    "- Name (Title) of post, for example \"The Toronto Raptors Win The...\"\n",
    "- The unique identifer of the post, for example: https://www.reddit.com/r/movies/comments/62sjuh/the_senate_upvote_this_so_that_people_see_it_when/\n",
    " would be 62sjuh. I am keeping this because I want to avoid repeats\n",
    "- The timestamp of the post as posts occur in chronological order and I don't want data leakage.\n",
    "\n",
    "As the aim of the project is to identify the subreddit only based on the title, I didn't scrape anything else. Also the non-text parts will be removed before running the model. They are just to make sure the data was collected properly.\n",
    "\n",
    "(It also avoids any posts that are NSFW because this is an assignment, if I wanted to actually develop a model I would leave them in since more data is better, although there aren't that many NSFW posts anyway.).\n",
    "\n",
    "\n",
    "I didn't record the subtext because a lot of the posts don't have any subtext, and I wanted to keep things consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that scrapes reddit using .json API\n",
    "\n",
    "def reddit_scraper(subreddit, save_loc, num_posts = 50):\n",
    "    \n",
    "    address = \"https://reddit.com/r/\" + subreddit + \".json\"\n",
    "    \n",
    "    # the guidelines for reddit api say to use this format for user agent\n",
    "    user_agent = \"Python:RedditScraping:1.0 (Test Scraper)\"\n",
    "    \n",
    "    # see how many pages based on # of posts\n",
    "    num_pages = num_posts//25\n",
    "    print('Ok, looping through {} pages'.format(num_pages))\n",
    "    \n",
    "    # set some things for the loop below\n",
    "    after = None\n",
    "    all_scraped_data = []\n",
    "    \n",
    "    # loop through for range \n",
    "    for i in range(num_pages):\n",
    "        \n",
    "        # get the correct address\n",
    "        if after is None:\n",
    "            full_address = address\n",
    "        else:\n",
    "            full_address = address + '?after=' + after\n",
    "            \n",
    "        # if i%10 == 0:\n",
    "        print(\"Page {} / {}\".format(i, num_pages))\n",
    "            \n",
    "        \n",
    "        # make time for sleeping\n",
    "        # I used poisson because it seems a bit more realistic\n",
    "        sleep_duration = np.random.poisson(30)\n",
    "        \n",
    "        # make sure it isn't TOO long or short\n",
    "        while sleep_duration > 60 or sleep_duration < 5:\n",
    "            sleep_duration = np.random.poisson(30)\n",
    "            \n",
    "        time.sleep(sleep_duration)\n",
    "\n",
    "        # send the req\n",
    "        req = requests.get(full_address, headers={\"User-agent\" : user_agent})\n",
    "        \n",
    "        \n",
    "        # see if the request is ok\n",
    "        if req.status_code != 200:\n",
    "            print(\"WARNING: STATUS IS NOT 200 ON {} ITERATION\".format(i))\n",
    "            print(\"Current 'after' is {}\".format(after))\n",
    "            continue\n",
    "            \n",
    "        # if request is ok, extract the json data:\n",
    "        current_json = req.json()\n",
    "        \n",
    "\n",
    "        # extract the things we want from the data\n",
    "        for s in current_json[\"data\"][\"children\"]:\n",
    "            data = s.get(\"data\")\n",
    "            \n",
    "            #check if it is NSFW, don't put it in if it is\n",
    "            is_nsfw =  data.get(\"thumbnail\", 0)            \n",
    "            if is_nsfw == \"nsfw\":\n",
    "                continue\n",
    "            \n",
    "            data_tuple = (data.get(\"title\"), data.get(\"name\"), data.get(\"created\"))\n",
    "            \n",
    "            # put it into the list\n",
    "            all_scraped_data.append(data_tuple)\n",
    "        \n",
    "        # get 'after' for the next loop\n",
    "        after = current_json[\"data\"][\"after\"]\n",
    "        \n",
    "        # Save data up to current loop as a .csv so the program is ok even if it crashes\n",
    "        pd.DataFrame(all_scraped_data).to_csv(save_loc)\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(\"yay it's done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the code lines below to run the scraper since it's very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, looping through 40 pages\n",
      "Page 0 / 40\n",
      "Page 1 / 40\n",
      "Page 2 / 40\n",
      "Page 3 / 40\n",
      "Page 4 / 40\n",
      "Page 5 / 40\n",
      "Page 6 / 40\n",
      "Page 7 / 40\n",
      "Page 8 / 40\n",
      "Page 9 / 40\n",
      "Page 10 / 40\n",
      "Page 11 / 40\n",
      "Page 12 / 40\n",
      "Page 13 / 40\n",
      "Page 14 / 40\n",
      "Page 15 / 40\n",
      "Page 16 / 40\n",
      "Page 17 / 40\n",
      "Page 18 / 40\n",
      "Page 19 / 40\n",
      "Page 20 / 40\n",
      "Page 21 / 40\n",
      "Page 22 / 40\n",
      "Page 23 / 40\n",
      "Page 24 / 40\n",
      "Page 25 / 40\n",
      "Page 26 / 40\n",
      "Page 27 / 40\n",
      "Page 28 / 40\n",
      "Page 29 / 40\n",
      "Page 30 / 40\n",
      "Page 31 / 40\n",
      "Page 32 / 40\n",
      "Page 33 / 40\n",
      "Page 34 / 40\n",
      "Page 35 / 40\n",
      "Page 36 / 40\n",
      "Page 37 / 40\n",
      "Page 38 / 40\n",
      "Page 39 / 40\n",
      "yay it's done\n",
      "--------------------\n",
      "Ok, looping through 40 pages\n",
      "Page 0 / 40\n",
      "Page 1 / 40\n",
      "Page 2 / 40\n",
      "Page 3 / 40\n",
      "Page 4 / 40\n",
      "Page 5 / 40\n",
      "Page 6 / 40\n",
      "Page 7 / 40\n",
      "Page 8 / 40\n",
      "Page 9 / 40\n",
      "Page 10 / 40\n",
      "Page 11 / 40\n",
      "Page 12 / 40\n",
      "Page 13 / 40\n",
      "Page 14 / 40\n",
      "Page 15 / 40\n",
      "Page 16 / 40\n",
      "Page 17 / 40\n",
      "Page 18 / 40\n",
      "Page 19 / 40\n",
      "Page 20 / 40\n",
      "Page 21 / 40\n",
      "Page 22 / 40\n",
      "Page 23 / 40\n",
      "Page 24 / 40\n",
      "Page 25 / 40\n",
      "Page 26 / 40\n",
      "Page 27 / 40\n",
      "Page 28 / 40\n",
      "Page 29 / 40\n",
      "Page 30 / 40\n",
      "Page 31 / 40\n",
      "Page 32 / 40\n",
      "Page 33 / 40\n",
      "Page 34 / 40\n",
      "Page 35 / 40\n",
      "Page 36 / 40\n",
      "Page 37 / 40\n",
      "Page 38 / 40\n",
      "Page 39 / 40\n",
      "yay it's done\n",
      "Done with both\n"
     ]
    }
   ],
   "source": [
    "# Scrape Subreddit 1\n",
    "subreddit = \"shittysuperpowers\"\n",
    "save_loc = \"../data/scraped_subreddit_1/hot_from_json.csv\"\n",
    "# reddit_scraper(subreddit, save_loc, num_posts = 1000)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Scrape Subreddit 2\n",
    "subreddit = \"godtiersuperpowers\"\n",
    "save_loc = \"../data/scraped_subreddit_2/hot_from_json.csv\"\n",
    "# reddit_scraper(subreddit, save_loc, num_posts = 1000)\n",
    "\n",
    "print(\"Done with both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW\n",
    "\n",
    "The above is very slow (although it works), so using PRAW (Python Reddit API Wrapper) is a lot faster. This also comes with other advantages:\n",
    "\n",
    "- Easier to test ideas\n",
    "- Seems to work better\n",
    "- Easier to use once set up correctly.\n",
    "- It's more 'correct' in that this is the method that Reddit intends for people to scrape data\n",
    "\n",
    "However it is still limited to 1000 posts. I decided to use the all-time top posts for this one since:\n",
    "- Bigger spread of posts over time (from the previous scraper, I found that 1000 posts filtered by hot is about 10 days)\n",
    "- Avoids scraping posts that may not match subreddit rules (for example, a post that doesn't fit the rules but hasn't been banned yet)\n",
    "\n",
    "- One drawback is that data from this may not be as good for predicting 'new' posts.\n",
    "- I don't want to scrape the same data again, obviously.\n",
    "\n",
    "I decided to use this (all-time top posts) as the data for the modelling process. For the above json data I'll just use it as the final test data, to see how well the model works.\n",
    "\n",
    "### IMPORTANT NOTE:\n",
    "Before running this make sure you have \"reddit_secrets.env\" in the main folder (i.e. Projects/project3/reddit_secrets.env\"). I should have sent this to you but if not please let me know. It contains the reddit username/secret/id and will be loaded in by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import praw\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# set the secret client id from environment variable\n",
    "load_dotenv(\"../reddit_secrets.env\")\n",
    "\n",
    "# get the client secret and password and username\n",
    "reddit_username = os.environ.get(\"REDDIT_USERNAME\")\n",
    "reddit_userid = os.environ.get(\"REDDIT_CLIENT_ID\")\n",
    "reddit_usersecret = os.environ.get(\"REDDIT_CLIENT_SECRET\")\n",
    "\n",
    "if (reddit_username is None) or (reddit_userid is None) or (reddit_usersecret is None):\n",
    "    print(\"You probably forgot to get the .env file\")\n",
    "\n",
    "# make user agent nicely formatted\n",
    "user_agent = \"jupyter:RedditScraper:v1.1 (by \" + reddit_username + \")\"\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = reddit_userid,\n",
    "    client_secret = reddit_usersecret,\n",
    "    user_agent = user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, actually scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that will scrape using PRAW\n",
    "def praw_scraper(subreddit, save_loc, num_posts = 50):\n",
    "    all_data = []\n",
    "    \n",
    "    #for submission in reddit.subreddit(subreddit).hot(limit=num_posts):\n",
    "    for submission in reddit.subreddit(subreddit).top(\"all\", limit=num_posts):\n",
    "        \n",
    "        # ignore if NSFW\n",
    "        if submission.over_18:\n",
    "            #pass\n",
    "            continue\n",
    "        \n",
    "        # get the data we want\n",
    "        one_post = (submission.title, submission.name, submission.created_utc)\n",
    "        \n",
    "        all_data.append(one_post)\n",
    "        \n",
    "    \n",
    "    # save it to the file\n",
    "    pd.DataFrame(all_data).to_csv(save_loc)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, uncomment the lines to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "--------------------\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "subreddit = \"shittysuperpowers\"\n",
    "save_loc = \"../data/scraped_subreddit_1/top_from_praw_1.csv\"\n",
    "# praw_scraper(subreddit, save_loc, num_posts = 1000)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "subreddit = \"godtiersuperpowers\"\n",
    "save_loc = \"../data/scraped_subreddit_2/top_from_praw_2.csv\"\n",
    "# praw_scraper(subreddit, save_loc, num_posts = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point please head back to '0_Main' notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
